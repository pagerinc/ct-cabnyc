{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The way agents are normally defined to make urban transportation decisions is given by the following formula:\n",
    "Transportation cost = out of pocket costs + accesibility costs + time. Users will choose commuting mode, living and work location and routes, according to how these choices impact their total transportation cost. This transportation cost is a standard way of defining \"efficiency\" in transportation. \n",
    "\n",
    "In our case, the out of pocket costs will be covered by the employee, and the accesibility costs will be held constant since we are only considering one mode of transportation (taxi cab). Thus, our user's choice of where to live (based on her potential commutes) will only be determined by the time of the commute, as it was stated in the [problem definition](https://github.com/pagerinc/ct-cabnyc/blob/master/README.md).\n",
    "\n",
    "I used [Rtrees](https://en.wikipedia.org/wiki/R-tree) to perform a spatial query on our data set that selects the trips that are _relevant_ for this analysis. Given the geographical polygons that encircle the areas where our subject would like to live, as well as the surroundings of her job location, relevant trips are defined as:\n",
    "\n",
    "* Those that are made between one of the three desired living areas and the work location, arriving between 8:50 and 9:00 am.\n",
    "* Those that are made between the work location and one of the desired areas, departing between 6:00 and 6:10 pm.\n",
    "\n",
    "After this, statistical summaries and tests will be provided to answer the question \"which home location would guarantee more *efficient* commutes?\". The [Data Science question](https://www.d.umn.edu/~kgilbert/ened5560-1/The%20Research%20Question-2015-Leek-1314-5.pdf) that we would be answering is that of exploratory analysis, since we will measure efficiency but do not quantify how our results would hold in a different sample.\n",
    "\n",
    "![data analysis flowchart](data_analysis_flowchart.png?raw=true \"Source: Jeffery T. Leek and Roger D.Peng for Science.\")\n",
    "\n",
    "Reproducibility has been taken into account and will be discused along the way. The whole process will look like this:\n",
    "\n",
    "### Process:\n",
    " 1. Loads and imports.\n",
    " 2. Load the geographical extent of the zones to be considered.\n",
    " 3. Load TLC data.\n",
    " 4. Filtering and aggregating relevant TLC data: filter times and origin-destination.\n",
    " 5. Choose the fastest commute in average, for a given monthly cap that the employee might provide.\n",
    " 6. Challenges.\n",
    " 7. Alternative solutions.\n",
    "\n",
    " _Note on reproducibility_ : Download the [TLC yellow cab data](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml) for 2016 (files are available from January to June) and store it without changing file names in a folder named \"Data\" inside this repository. I chose to do the file loading this way instead of doing an URL call directly from this code, so I could keep the data in my own machine and not have to go through this very time consuming process every time I restarted my the Python kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loads and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducibility note: be sure to install the following packages. Notice that \"spatial\" is a module that comes with this solution (spatial.py). More details will be provided about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import smopy\n",
    "import json\n",
    "# Usually not recommended to import * but this module is quite small. Only \"Points\", \"Polygons\" and \"contains\" belong to it.\n",
    "from spatial import * \n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load geographic extent of the zones to be considered\n",
    "\n",
    "In the following cell I will do one of two things to define polygons surrounding the areas that are relevant for spatial querying:\n",
    "\n",
    "* Use the google Maps API to define the geographical boundaries of the areas that were successfully queried by this method, and then using the \"Polygon\" class of the \"Spatial\" module with these boundaries as parameters.\n",
    "* Manually define the polygons that are not well defined by a Google Maps API call.\n",
    "\n",
    "### Challenges and limitations:\n",
    "* Boundaries in real life, particularly in this kind of urban problem, are more \"fuzzy\". This means that, for example, people who live in the defined areas will not necesarily get on cabs only within the areas that are defined by a name, such as \"Sutton Place\". People living inside these areas could usually get on cabs one block or two feet away from that boundary, for example.\n",
    "\n",
    "* The TLC data set is very useful, among many other reasons, due to its size. At the end of this excercise I just get a ver small, almost insignificant subset with the method that I am proposing. A solution for this would be to expand the geographic extent of the defined polygons, and a good reason for doing is the \"fuzzyness\" of the areas that is mentioned in the previous comment.\n",
    "\n",
    "* I only get to manually define polygons that are not given by the API because the number of zones that need this treatment is small. Otherwise a more rigorous, error-proof approach would be needed. An alternative would be:\n",
    "    * When the Google Maps API response is a point and not a polygon (which is the case for Lincoln center in our example), we could ask a GIS service to draw polygon buffers around that point. That GIS service could be Open Street Maps (which can be used from its API), or even ArcGIS (with some inner Python code for automation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:requests.packages.urllib3.contrib.pyopenssl:A problem was encountered with the certificate that prevented urllib3 from finding the SubjectAlternativeName field. This can affect certificate validation. The error was Codepoint U+002A at position 1 of u'*' not allowed\n",
      "/home/saf537/anaconda2/lib/python2.7/site-packages/requests/packages/urllib3/connection.py:337: SubjectAltNameWarning: Certificate for maps.googleapis.com has no `subjectAltName`, falling back to check for a `commonName` for now. This feature is being removed by major browsers and deprecated by RFC 2818. (See https://github.com/shazow/urllib3/issues/497 for details.)\n",
      "  SubjectAltNameWarning\n",
      "WARNING:requests.packages.urllib3.contrib.pyopenssl:A problem was encountered with the certificate that prevented urllib3 from finding the SubjectAlternativeName field. This can affect certificate validation. The error was Codepoint U+002A at position 1 of u'*' not allowed\n",
      "/home/saf537/anaconda2/lib/python2.7/site-packages/requests/packages/urllib3/connection.py:337: SubjectAltNameWarning: Certificate for maps.googleapis.com has no `subjectAltName`, falling back to check for a `commonName` for now. This feature is being removed by major browsers and deprecated by RFC 2818. (See https://github.com/shazow/urllib3/issues/497 for details.)\n",
      "  SubjectAltNameWarning\n"
     ]
    }
   ],
   "source": [
    "googleKey = 'AIzaSyBvuKUfCCTNzc8etkAuaU-16uzl3N4f6Vw' # Hide for privacy.\n",
    "\n",
    "#-- Two bridges\n",
    "# API call and filling polygon vertices with its response.\n",
    "Zone_A = 'Two Bridges'\n",
    "call = 'https://maps.googleapis.com/maps/api/geocode/json?address='+Zone_A+'&components=locality:New York&key='+googleKey\n",
    "rep_ZA = json.loads(requests.get(call).content);\n",
    "ZA_lat_NE = rep_ZA['results'][0]['geometry']['bounds'][u'northeast'][u'lat']\n",
    "ZA_lng_NE = rep_ZA['results'][0]['geometry']['bounds'][u'northeast'][u'lng']\n",
    "ZA_lat_SW = rep_ZA['results'][0]['geometry']['bounds'][u'southwest'][u'lat']\n",
    "ZA_lng_SW = rep_ZA['results'][0]['geometry']['bounds'][u'southwest'][u'lng']\n",
    "\n",
    "Zone_A = Polygon([Point(ZA_lng_NE,ZA_lat_NE),\n",
    "               Point(ZA_lng_NE,ZA_lat_SW),\n",
    "               Point(ZA_lng_SW,ZA_lat_NE),\n",
    "               Point(ZA_lng_SW,ZA_lat_SW)\n",
    "              ])\n",
    "\n",
    "#-- Lincoln center. Manually added.\n",
    "# Manually filling polygon vertices.\n",
    "Zone_B = Polygon([Point(-73.981531, 40.768625),\n",
    "              Point(-73.992835, 40.773397),\n",
    "              Point(-73.989141, 40.776848),\n",
    "              Point(-73.978333, 40.773019),\n",
    "              Point(-73.981531, 40.768625)\n",
    "             ])\n",
    "\n",
    "#-- Sutton Place\n",
    "# API call and filling polygon vertices with its response.\n",
    "Zone_C = 'Sutton Place'\n",
    "call = 'https://maps.googleapis.com/maps/api/geocode/json?address='+Zone_C+'&components=locality:New York&key='+googleKey\n",
    "rep_ZC = json.loads(requests.get(call).content);\n",
    "ZC_lat_NE = rep_ZC['results'][0]['geometry']['bounds'][u'northeast'][u'lat']\n",
    "ZC_lng_NE = rep_ZC['results'][0]['geometry']['bounds'][u'northeast'][u'lng']\n",
    "ZC_lat_SW = rep_ZC['results'][0]['geometry']['bounds'][u'southwest'][u'lat']\n",
    "ZC_lng_SW = rep_ZC['results'][0]['geometry']['bounds'][u'southwest'][u'lng']\n",
    "\n",
    "Zone_C = Polygon([Point(ZC_lng_NE,ZC_lat_NE),\n",
    "               Point(ZC_lng_NE,ZC_lat_SW),\n",
    "               Point(ZC_lng_SW,ZC_lat_NE),\n",
    "               Point(ZC_lng_SW,ZC_lat_SW)\n",
    "              ])\n",
    "\n",
    "#-- Two blocks around '1 Irving Pl, New York, NY 10003'\n",
    "# Many\n",
    "work = Polygon([Point(-73.988747, 40.733891),\n",
    "                Point(-73.987652, 40.735385),\n",
    "                Point(-73.989885, 40.735341),\n",
    "                Point(-73.990882, 40.734815),\n",
    "                Point(-73.988747, 40.733891)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the trips to be analized.\n",
    "\n",
    "In the following cell I define functions that return boolean arrays for each data point in our data set, answering to the questions:\n",
    "* Does this trip start in one of the three defined zones and ends around the workplace location between 8:50 am and 9:00 am?\n",
    "* Does this trip start in the workplace between 6:00 and 6:10 pm and ends in one of the defined areas?\n",
    "\n",
    "In the next cell I define a function that returns the rows of the data set for which these functions return true values. The reason for defining this function will be explained below.\n",
    "\n",
    "### Limitations:\n",
    "This is a very time consuming technique, since I am using vectorized functions to do queries on the data sets. Probably using lambda functions or SQL-like statements in the queries would have been less time (and code lines) consuming. Nevertheless, since the main interest of my solution is not computational time efficiency, this is left for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zone_A_subset(lon,lat):\n",
    "    pt = Point(lon,lat)\n",
    "    question = Zone_A.contains(pt)\n",
    "    return question\n",
    "\n",
    "def zone_B_subset(lon,lat):\n",
    "    pt = Point(lon,lat)\n",
    "    question = Zone_B.contains(pt)\n",
    "    return question\n",
    "\n",
    "def zone_C_subset(lon,lat):\n",
    "    pt = Point(lon,lat)\n",
    "    question = Zone_C.contains(pt)\n",
    "    return question\n",
    "\n",
    "def work_subset(lon,lat):\n",
    "    pt = Point(lon,lat)\n",
    "    question = work.contains(pt)\n",
    "    return question\n",
    "\n",
    "def time_morning(S):\n",
    "    # Dropoff\n",
    "    A = pd.Timestamp(S)\n",
    "    return ((A.time() > dt.time(8,50,0)) & (A.time() < dt.time(9,00,00)))\n",
    "\n",
    "def time_afternoon(S):\n",
    "    # Pickup\n",
    "    A = pd.Timestamp(S)\n",
    "    return ((A.time() > dt.time(18,00,0)) & (A.time() < dt.time(18,15,0)))\n",
    "\n",
    "# -- vectorize all of our functions, so they work for an entire column instead of point by point.\n",
    "vfunc_A    = np.vectorize(zone_A_subset)\n",
    "vfunc_B    = np.vectorize(zone_B_subset)\n",
    "vfunc_C    = np.vectorize(zone_C_subset)\n",
    "vfunc_work = np.vectorize(work_subset)\n",
    "vfunc_time_morning   = np.vectorize(time_morning)\n",
    "vfunc_time_afternoon = np.vectorize(time_afternoon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -- Function that queries relevant trips and adds relevant columns: Zone and time of the commute.\n",
    "\n",
    "def filter_db(taxi_records):\n",
    "    # From HOME to WORK\n",
    "    morning_A = taxi_records[((vfunc_A(taxi_records['pickup_longitude'],taxi_records['pickup_latitude'])==True) \n",
    "                              & (vfunc_work(taxi_records['dropoff_longitude'],taxi_records['dropoff_latitude'])==True)\n",
    "                              & (vfunc_time_morning(taxi_records['tpep_dropoff_datetime'])==True))]\n",
    "    morning_A['Zone'] = 'Two Bridges'\n",
    "    morning_A['Commute'] = 'Morning'\n",
    "\n",
    "    morning_B = taxi_records[((vfunc_B(taxi_records['pickup_longitude'],taxi_records['pickup_latitude'])==True) \n",
    "                              & (vfunc_work(taxi_records['dropoff_longitude'],taxi_records['dropoff_latitude'])==True)\n",
    "                              & (vfunc_time_morning(taxi_records['tpep_dropoff_datetime'])==True))]\n",
    "    morning_B['Zone'] = 'Lincoln Center'\n",
    "    morning_B['Commute'] = 'Morning'\n",
    "\n",
    "    morning_C = taxi_records[((vfunc_C(taxi_records['pickup_longitude'],taxi_records['pickup_latitude'])==True) \n",
    "                              & (vfunc_work(taxi_records['dropoff_longitude'],taxi_records['dropoff_latitude'])==True)\n",
    "                              & (vfunc_time_morning(taxi_records['tpep_dropoff_datetime'])==True))]\n",
    "    morning_C['Zone'] = 'Sutton Place'\n",
    "    morning_C['Commute'] = 'Morning'\n",
    "\n",
    "    # From WORK to HOME\n",
    "    afternoon_A = taxi_records[((vfunc_A(taxi_records['dropoff_longitude'],taxi_records['dropoff_latitude'])==True) \n",
    "                              & (vfunc_work(taxi_records['pickup_longitude'],taxi_records['pickup_latitude'])==True)\n",
    "                              & (vfunc_time_afternoon(taxi_records['tpep_pickup_datetime'])==True))]\n",
    "    afternoon_A['Zone'] = 'Two Bridges'\n",
    "    afternoon_A['Commute'] = 'Afternoon'\n",
    "\n",
    "\n",
    "    afternoon_B = taxi_records[((vfunc_B(taxi_records['dropoff_longitude'],taxi_records['dropoff_latitude'])==True) \n",
    "                              & (vfunc_work(taxi_records['pickup_longitude'],taxi_records['pickup_latitude'])==True)\n",
    "                              & (vfunc_time_afternoon(taxi_records['tpep_pickup_datetime'])==True))]\n",
    "    afternoon_B['Zone'] = 'Lincoln Center'\n",
    "    afternoon_B['Commute'] = 'Afternoon'\n",
    "\n",
    "\n",
    "    afternoon_C =  taxi_records[((vfunc_B(taxi_records['dropoff_longitude'],taxi_records['dropoff_latitude'])==True) \n",
    "                              & (vfunc_work(taxi_records['pickup_longitude'],taxi_records['pickup_latitude'])==True)\n",
    "                              & (vfunc_time_afternoon(taxi_records['tpep_pickup_datetime'])==True))]\n",
    "\n",
    "    afternoon_C['Zone'] = 'Sutton Place'\n",
    "    afternoon_C['Commute'] = 'Afternoon'\n",
    "\n",
    "    taxi_filtered = pd.concat([morning_A, morning_B, morning_C, afternoon_A, afternoon_B, afternoon_C])\n",
    "    taxi_filtered['trip_duration'] = (taxi_filtered['tpep_dropoff_datetime'].astype('datetime64') - taxi_filtered['tpep_pickup_datetime'].astype('datetime64[m]')).astype('timedelta64[m]')\n",
    "    return taxi_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load TLC data and 4. Filtering and aggregating relevant TLC data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I just load the TLC data set month by month, and select only the relevant data in each iteration, since loading and appending the complete monhtly data sets would be innecesary since we are only choosing a very small fraction of it.\n",
    "\n",
    "### Challenges and limitations\n",
    "* I am appending everything into a single Pandas DataFrame named \"taxi_filtered\". There is a warning that comes from the fact that I redefine that Data Frame in each iteration. This should certainly take a toll on computational time (and looks ugly). \n",
    "* A simple solution to this (that can be easily implemented in short time) is to append each Data Frame into a list and concatenate them into a single Data Frame outside of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saf537/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/saf537/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#-- Load data. Do NOT run this cell if you do not want to wait a LONG time.\n",
    "\n",
    "taxi_filtered = pd.DataFrame([])\n",
    "for i in range(1,7):\n",
    "    path= 'Data/yellow_tripdata_2016-0'+str(i)+'.csv'\n",
    "    taxi_records = pd.read_csv(path,usecols=['tpep_pickup_datetime','tpep_dropoff_datetime','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude'])\n",
    "    taxi_filtered = taxi_filtered.append(filter_db(taxi_records))\n",
    "    del(taxi_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taxi_filtered.to_csv('selected_trips_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('The amount of queried data points for our analysis is: {}'.format(len(taxi_filtered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 5. Choose the likely-to-be the most efficient commute.\n",
    "The following is an exploratory analysis on the differences between the commutes from and to each of the candidates.\n",
    "\n",
    "### Commute times between zones.\n",
    "The distribution of commute times for each zone is illustrated by the following plot and summary table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taxi_filtered.boxplot('trip_duration',by='Zone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taxi_filtered.groupby('Zone').agg(['mean','median','std'])['trip_duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, it can be observed that:\n",
    "* Two Bridges has a the highest standard deviation (more than 50% of the mean).\n",
    "* Sutton Place has the lowest average and median commute times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commute time difference by time of the day.\n",
    "\n",
    "But notice that there is a big difference between the morning and afternoon commute. According to the following boxplot and summary table, the afternoon trip tends to be slower and more uncertain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taxi_filtered.boxplot('trip_duration',by='Commute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taxi_filtered.groupby('Commute').agg(['mean','median','std'])['trip_duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the following table it can be seen that the afternoon trip tends to be better for those who live in Two Bridges, whereas the morning commute is comparably good for those who come from Sutton place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taxi_filtered.groupby(['Commute','Zone']).agg(['mean','median','std'])['trip_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taxi_filtered.query('Commute == \"Morning\"').boxplot('trip_duration',by='Zone')\n",
    "plt.title('Commute differences between zones in the morning.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taxi_filtered.query('Commute == \"Afternoon\"').boxplot('trip_duration',by='Zone')\n",
    "plt.title('Commute differences between zones in the afternoon.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A definition of efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People like to minimize their commute times, but they *hate* to have large uncertainties about their trip times because this takes away their capability to plan ahead successfully. For this reason, let us think about efficiency as the opposite of agony, which we can define as:\n",
    "\n",
    "$$Agony = Average(time) + Std(time)$$\n",
    "\n",
    "This way, larger commute times as well as larger standard deviations will make for more agonic -less efficient- trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = taxi_filtered.groupby('Zone')\n",
    "Agony = grouped.mean()['trip_duration'] + grouped.std()['trip_duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this criterion, our model would choose the less agonic choice, i.e., living in Sutton Place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Agony"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taxi_filtered[taxi_filtered['Zone']=='Two Bridges'].hist('trip_duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taxi_filtered[taxi_filtered['Zone']=='Lincoln Center'].hist('trip_duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taxi_filtered[taxi_filtered['Zone']=='Sutton Place'].hist('trip_duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 6. Thoughts and assumptions.\n",
    " \n",
    "* This exercise ignores the use of shared car companies like Uber. Althought they might be more efficient in terms of out of pocket costs (their service is presumably just as fast, although this is topic for another analysis), it can be easily assumed that, on average, the location that allows for the fastest trips by taxi, will also be the one from which faster shared car services are faster.\n",
    " \n",
    "* An interesting extension to this excercise would be to assume (realistically) that the employer sets a limit on the amount of money she would be willing to invest in financing the employee's commutes. In this case, the trip cost would also be added to the efficiency equation, penalizing more expensive trips.\n",
    " \n",
    "* Efficiency of the code and language choices: if this process were to be automated, I would *NOT* use Ipython Notebooks. I chose this tool because of its usefulness for documenting and illustrating a thought process. SQL is a great alternative for data querying, including the use of spatial R Trees.\n",
    "\n",
    "* In order to optimize speed, I would have chosen to use big data techniques as the ones used [here](https://github.com/sarangof/Citibike-vs-taxis-NYC) and [here](https://github.com/sarangof/Bus-Capstone) (Map Reduce on Hadoop, Spark).\n",
    "\n",
    "* If I were to do this again, I would choose larger time windows and geographical extents, in order to enlarge the sample size.\n",
    "\n",
    "* In this case I am only choosing between 3 options, which is relatively simple to do visually. This excercise becomes much more interesting when more choices come to play.\n",
    "\n",
    "* Visualization: a geographical visualization of this exercise would consist on coloring the polygons (using a sequential scale) according to the calculated efficiencies (or agonies) on top of a basemap. This c\n",
    "\n",
    "# HERE\n",
    "\n",
    "* A more rigurous analysis would test for normality of the distributions (this would help us decide if we should care more about means or medians), and possibly make ANOVA tests to check for true differences between groups (zones/areas in our case).\n",
    "\n",
    "* I realized that (mostly) on afternoon trips, it is quite likely that employeers share taxi rides with coworkers. The TLC data set provides a passenger count field, that should be used in the future, since this is a potential source of error of my analysis.\n",
    " \n",
    " # 8. Alternative solutions.\n",
    " \n",
    "* [The American Comunity Survey (ACS)](https://www.census.gov/programs-surveys/acs/) publishes [estimations](https://www.socialexplorer.com/data/ACS2014/metadata/?ds=American+Community+Survey+2014&table=B08134) of the average time that people spend commuting from each block group. The Longitudinal Employer-Household Dynamics [LEHD](https://lehd.ces.census.gov/data/) data service provides the number of people who commute between block groups in the city. Both sources can be very useful (see [this project](https://github.com/sarangof/transport-Equality_NYC)) but sadly the level of aggregation does not for estimations of time *between* the block groups where our zones of interest would be located. This approach would have the limitations that come with using this level of granularity (error, arbitrariness of the boundaries), very similar to our approach, along with the error that surveys and estimations carry, since this information is usually extracted from projections that are based on the decenial census.\n",
    "\n",
    "* Real Estate information would be valuable (see [zillow.com](https://www.zillow.com)) if we wanted to give our client more information given her rent budget. Again, this poses a big question on the right aggregation level. A reasonable alternative would be to scrape \n",
    "\n",
    "window methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
